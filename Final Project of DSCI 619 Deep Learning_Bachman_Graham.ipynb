{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - DSCI 619 Deep Learning\n",
    "### Graham Bachman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Image Classification\n",
    "\n",
    "Data Source: https://www.kaggle.com/puneet6060/intel-image-classification\n",
    "\n",
    "+ buildings\n",
    "+ forest\n",
    "+ glacier\n",
    "+ mountain\n",
    "+ sea\n",
    "+ street\n",
    "\n",
    "The dataset has been divided into folders for training, testing, and prediction. The training folder includes around 14,000 images and the testing folder has around 3,000 images. Finally, the prediction folder includes around 7,000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q1 here and run this cell to produce the results.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='../input/intel-image-classification/seg_train/seg_train',\n",
    "    image_size=(128,128),\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=128\n",
    ")\n",
    "\n",
    "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='../input/intel-image-classification/seg_train/seg_train',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    image_size=(128,128),\n",
    "    seed=128\n",
    ")\n",
    "\n",
    "test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='../input/intel-image-classification/seg_test/seg_test',\n",
    "    image_size=(128,128),\n",
    "    seed=128\n",
    ")\n",
    "\n",
    "print()\n",
    "print('Classes: ', train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CNN model on the training dataset and evaluate it on the test dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q2 here and run this cell to produce the results.\n",
    "normal = train.map(lambda x, y: (layers.experimental.preprocessing.Rescaling(1./255)(x), y))\n",
    "image_batch, labels_batch = next(iter(normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q1 here and run this cell to produce the results.\n",
    "model_1 = Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(128, 128, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "    \n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "    \n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  \n",
    "  layers.Flatten(),\n",
    "  \n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(6)\n",
    "])\n",
    "\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_1 = model_1.fit(train, validation_data=val, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_1.evaluate(test)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = log_1.history['accuracy']\n",
    "val_acc = log_1.history['val_accuracy']\n",
    "\n",
    "loss = log_1.history['loss']\n",
    "val_loss = log_1.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(10), acc, label='Training Accuracy')\n",
    "plt.plot(range(10), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(10), loss, label='Training Loss')\n",
    "plt.plot(range(10), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model 2, with data augmentation and dropout and evaluate it on the test dataset.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your code for Q5 here. Please run this cell and produce results.\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(128, 128, 3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential([\n",
    "data_augmentation,\n",
    "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(128, 128, 3)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(6)\n",
    "    ])\n",
    "\n",
    "model_2.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_2 = model_2.fit(train, validation_data=val, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_2.evaluate(test)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = log_2.history['accuracy']\n",
    "val_acc = log_2.history['val_accuracy']\n",
    "\n",
    "loss = log_2.history['loss']\n",
    "val_loss = log_2.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(10), acc, label='Training Accuracy')\n",
    "plt.plot(range(10), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(10), loss, label='Training Loss')\n",
    "plt.plot(range(10), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model 3, based on the pre-trained model (transfer learning) and evaluate it on the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q4 here and run this cell to produce the results.\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "image_batch, label_batch = next(iter(train))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)\n",
    "prediction_layer = tf.keras.layers.Dense(6)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(128, 128, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model_3.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model_3.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_3 = model_3.fit(train,\n",
    "                    epochs=20,\n",
    "                    validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_3.evaluate(test)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = log_3.history['accuracy']\n",
    "val_acc = log_3.history['val_accuracy']\n",
    "\n",
    "loss = log_3.history['loss']\n",
    "val_loss = log_3.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(10), acc, label='Training Accuracy')\n",
    "plt.plot(range(10), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(10), loss, label='Training Loss')\n",
    "plt.plot(range(10), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.  Which model do you recommend for the model in Q2, Q3, and Q4? Justify your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend the third model, because it scored the best on the test and validation sets and showed little evidence of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Sentiment Analysis\n",
    "\n",
    "Data Source: https://www.kaggle.com/hj5992/restaurantreviews\n",
    "\n",
    "The dataset is comprised of tab-separated files with two columns:\n",
    "\n",
    "+ Review: Customers' review \n",
    "+ Liked : 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q1 here and run this cell to produce the results.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "import os\n",
    "import re #regular expression, easier for us to clean texts\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # map text to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/restaurantreviews/Restaurant_Reviews.tsv', sep='\\t', quoting=3)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "           ( tf.cast(df['Review'].values, tf.string),\n",
    "            tf.cast(df['Liked'].values, tf.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and preprocess the text data and split into training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q2 here and run this cell to produce the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=1, random_state=42)\n",
    "\n",
    "train_ds, val_ds, test_dataset = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])\n",
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(len(dataset)*0.7)\n",
    "\n",
    "train_dataset = dataset.take(TRAIN_SIZE)\n",
    "test_dataset = dataset.skip(TRAIN_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 1\n",
    " \n",
    "train_dataset = train_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline RNN model using embedding layer and GRU on the training dataset and evaluate it on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q3 here and run this cell to produce the results.\n",
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()), \n",
    "        output_dim=64, \n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = model_1.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_1.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None,1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN model 2 using embedding layer and LSTM and evaluate it on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q4 here and run this cell to produce the results.\n",
    "model_2 = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()), \n",
    "        output_dim=64, \n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = model_2.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_2.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history_2, 'accuracy')\n",
    "plt.ylim(None,1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history_2, 'loss')\n",
    "plt.ylim(0,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN model 3, using embedding layer and GRU and LSTM and evaluate it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put your codes for Q5 here and run this cell to produce the results.\n",
    "model_3 = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()), \n",
    "        output_dim=64, \n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_3 = model_3.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_3.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plot_graphs(history_3, 'accuracy')\n",
    "plt.ylim(None,1)\n",
    "plt.subplot(1,2,2)\n",
    "plot_graphs(history_3, 'loss')\n",
    "plt.ylim(0,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would recommend the first because it had the best accuracy. All three models were overfitting significantly, and more work needs to be done to remedy this. My first approach would be to slow the learning rate and train over more epochs. It would also be beneficial to include more Dropout Layers. But my recommendation for now is the first model, on the grounds of it's performance on the test data set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
